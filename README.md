# Diffusion and Flow Models Research

This repository curates research on Diffusion and Flow-based models, offering a structured overview of key concepts, advancements, and applications in generative AI. It covers diverse topics from inference acceleration and model optimization to image editing and personalized generation. Contributions are welcome to keep this resource comprehensive and up-to-date.

## Contents

- [Inference Acceleration](#inference-acceleration)
- [Quantization](#quantization)
- [Optimization](#optimization)
- [Model Distillation](#model-distillation)
- [Latent Domain Applications](#latent-domain-applications)
- [Flow Matching Concepts](#flow-matching-concepts)
- [Diffusion Concepts](#diffusion-concepts)
- [Image Editing](#image-editing)
- [Personalized - ID Preserving Image Generation](#personalized---id-preserving-image-generation)

## ‚ö°Ô∏è Inference Acceleration

**Training-free Diffusion Acceleration with Bottleneck Sampling** \
[[Paper](https://arxiv.org/pdf/2503.18940)]
[[Project Page](https://tyfeld.github.io/BottleneckSampling.github.io/)]

**NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers** \
[[Paper](https://arxiv.org/pdf/2503.09242)]


**SVDQUANT: ABSORBING OUTLIERS BY LOW-RANK COMPONENTS FOR 4-BIT DIFFUSION MODELS** \
[[Paper](https://arxiv.org/pdf/2411.05007)]

**From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers** \
[[Paper](https://arxiv.org/pdf/2503.06923)]
[[Project Page](https://taylorseer.github.io/TaylorSeer/)]

## ü§è Quantization

**SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION** \
[[Paper](https://arxiv.org/pdf/2410.02367)]
[[Project Page](https://github.com/thu-ml/SageAttention)]

**SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization** \
[[Paper](https://arxiv.org/pdf/2411.10958)]


## ‚öôÔ∏è Model Optimization

**FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** \
[[Paper](https://arxiv.org/pdf/2205.14135)]
[[Project Page](https://github.com/Dao-AILab/flash-attention.git)]

**FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning** \
[[Paper](https://tridao.me/publications/flash2/flash2.pdf)]

**FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision** \
[[Paper](https://tridao.me/publications/flash3/flash3.pdf)]



## üß™ Model Distillation

**SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation** \
[[Paper](https://arxiv.org/pdf/2503.09641)]
[[Project Page](https://github.com/NVlabs/Sana)]


## ‚ú® Latent Domain Applications


## üåä Flow Matching Concepts

**FLOW MATCHING FOR GENERATIVE MODELING** \
[[Paper](https://arxiv.org/pdf/2210.02747)]


## üåÄ Diffusion Concepts


## üé® Image Editing

**FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing** \
[[Paper](https://arxiv.org/pdf/2412.07517)]
[[Project Page](https://github.com/HolmesShuan/FireFlow-Fast-Inversion-of-Rectified-Flow-for-Image-Semantic-Editing)]

**FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models** \
[[Paper](https://arxiv.org/pdf/2412.08629)]
[[Project Page](https://matankleiner.github.io/flowedit/)]



## üë§ Personalized - ID Preserving Image Generation

**InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity** \
[[Paper](https://arxiv.org/pdf/2503.16418)]
[[Project Page](https://bytedance.github.io/InfiniteYou/)]

**LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers** \
[[Paper](https://arxiv.org/pdf/2505.23758)]
[[Project Page](https://lorashop.github.io)]

**FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers** \
[[Paper](https://arxiv.org/pdf/2412.096118)]


