# Diffusion and Flow Models Research

This repository curates research on Diffusion and Flow-based models, offering a structured overview of key concepts, advancements, and applications in generative AI. It covers diverse topics from inference acceleration and model optimization to image editing and personalized generation. Contributions are welcome to keep this resource comprehensive and up-to-date.

## Contents

- [Inference Acceleration](#inference-acceleration)
- [Quantization](#quantization)
- [Optimization](#optimization)
- [Model Distillation](#model-distillation)
- [Evaluation Metrics](#evaluation-metrics)
- [Latent Domain Applications](#latent-domain-applications)
- [Flow Matching Concepts](#flow-matching-concepts)
- [Diffusion Concepts](#diffusion-concepts)
- [Image Editing](#image-editing)
- [Personalized - ID Preserving Image Generation](#personalized---id-preserving-image-generation)
- [Object Based Generation](#object-based-generation)

## ‚ö°Ô∏è Inference Acceleration

**Training-free Diffusion Acceleration with Bottleneck Sampling** \
[[Paper](https://arxiv.org/pdf/2503.18940)]
[[Project Page](https://tyfeld.github.io/BottleneckSampling.github.io/)]

**NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers** \
[[Paper](https://arxiv.org/pdf/2503.09242)]


**SVDQUANT: ABSORBING OUTLIERS BY LOW-RANK COMPONENTS FOR 4-BIT DIFFUSION MODELS** \
[[Paper](https://arxiv.org/pdf/2411.05007)]



## ü§è Quantization

**SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION** \
[[Paper](https://arxiv.org/pdf/2410.02367)]
[[Project Page](https://github.com/thu-ml/SageAttention)]

**SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization** \
[[Paper](https://arxiv.org/pdf/2411.10958)]


## ‚öôÔ∏è Model Optimization

**FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** \
[[Paper](https://arxiv.org/pdf/2205.14135)]
[[Project Page](https://github.com/Dao-AILab/flash-attention.git)]

**FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning** \
[[Paper](https://tridao.me/publications/flash2/flash2.pdf)]

**FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision** \
[[Paper](https://tridao.me/publications/flash3/flash3.pdf)]



## üß™ Model Distillation


## üìä Evaluation Metrics


## ‚ú® Latent Domain Applications


## üåä Flow Matching Concepts

**FLOW MATCHING FOR GENERATIVE MODELING** \
[[Paper](https://arxiv.org/pdf/2210.02747)]


## üåÄ Diffusion Concepts


## üé® Image Editing


## üë§ Personalized - ID Preserving Image Generation

**InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity** \
[[Paper](https://arxiv.org/pdf/2503.16418)]
[[Project Page](https://bytedance.github.io/InfiniteYou/)]

## üì¶ Object Based Generation
